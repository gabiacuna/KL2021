{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "001-Autograd-Tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabiacuna/KL2021/blob/main/Analisis%20de%20Imagenes%20con%20DL/001-Autograd-Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-C2brKaMZSz"
      },
      "source": [
        "**Fecha** : 2021-06-28\n",
        "\n",
        "# Autograd: Diferenciación automática\n",
        "\n",
        "El paquete `autograd` proporciona una diferenciación automática para todas las operaciones\n",
        "en tensores. Es un marco de definición por ejecución, lo que significa que su backprop es\n",
        "definido por cómo se ejecuta su código, y que cada iteración puede ser\n",
        "diferente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5venE7WMZS2"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aBW2mB2MZS4"
      },
      "source": [
        "Creando a un tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCPzARaAMZS5",
        "outputId": "c61c30a4-45b1-4609-c83a-aeb42e75f3f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cree un tensor 2x2 con capacidades de acumulación de gradientes\n",
        "x = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTGuamPlMZS6"
      },
      "source": [
        "Ahora podemos hacer operaciónes sobre los tensores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SXuUYPoMZS7",
        "outputId": "06510e91-9a27-4768-be4f-7f2006cfb2d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Deduct 2 from all elements\n",
        "y = x - 2\n",
        "print(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.,  0.],\n",
            "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzi269xSMZS7"
      },
      "source": [
        "`y` fue creada como resultado de una operación, y por ello tiene ele atributo `grad_fn`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlWcdxU_MZS8",
        "outputId": "03cbd8d9-c556-4646-c96a-032af2e8fe56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SubBackward0 object at 0x7fb03cc29350>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRFkh5HPMZS9",
        "outputId": "6e55b0bc-7720-4874-9989-738221a7d112",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ¿Qué pasa acá?\n",
        "print(x.grad_fn)    #Sale none pok x no depende de y, solo tiene requires_grad=True"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RET29EYMZS-",
        "outputId": "268a47d8-0dc9-4b2e-dabb-121bec41c2a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's dig further...\n",
        "y.grad_fn"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SubBackward0 at 0x7fb03d1666d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxXxOJWBMZS-",
        "outputId": "4e3e1a09-82ef-47ba-be46-de73d58a3ba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y.grad_fn.next_functions[0][0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AccumulateGrad at 0x7fb03cc0ad90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bvcOVDXMZS-",
        "outputId": "58a85d1e-8d64-4ca5-de57-d8efa060b5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y.grad_fn.next_functions[0][0].variable"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj-jZZTfMZS_"
      },
      "source": [
        "# Podemos aplicar más funciones sobre `y`\n",
        "z = y * y * 3\n",
        "a = z.mean()  # promedio\n",
        "\n",
        "print(z)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arcFMSx4MZS_"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "Ahora podemos retroceder con `out.backward()`, lo cual es equivalente a\n",
        "`out.backward(torch.tensor([1.0]))`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKbjm0s2MZS_"
      },
      "source": [
        "# Backprop\n",
        "a.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jq69UEFMZTA"
      },
      "source": [
        "Print gradients $\\frac{\\text{d}a}{\\text{d}x}$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adiOqlMzMZTA"
      },
      "source": [
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wdblDCFMZTA"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "i = 0\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "    i += 1\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFCPQm9mMZTA"
      },
      "source": [
        "# Si no corremos hacia atrás en un escalar, necesitamos especificar el grad_output\n",
        "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
        "y.backward(gradients)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L89hnleiMZTB"
      },
      "source": [
        "print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oIO9CVSMZTB"
      },
      "source": [
        "## Inferencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buCRtz7UMZTB"
      },
      "source": [
        "# Esta variable decide el rango del tensor por debajo de\n",
        "n = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTFC6U0cMZTB"
      },
      "source": [
        "# Tanto `x` como `w` que permiten la acumulación de gradientes.\n",
        "x = torch.arange(1., n + 1, requires_grad=True)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "z = w @ x\n",
        "z.backward()\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EszKBn1qMZTC"
      },
      "source": [
        "# Solo `w` que permite la acumulación de gradiente\n",
        "x = torch.arange(1., n + 1)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "z = w @ x\n",
        "z.backward()\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1oO0BowMZTC"
      },
      "source": [
        "x = torch.arange(1., n + 1)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "\n",
        "# Independientemente de lo que haga en este contexto, todos los tensores de la\n",
        "# antorcha no tendrán acumulación de gradiente\n",
        "with torch.no_grad():\n",
        "    z = w @ x\n",
        "\n",
        "try:\n",
        "    z.backward()  # PyTorch arrojará un error aquí, ya que z no tiene acumulaciones graduales.\n",
        "except RuntimeError as e:\n",
        "    print('RuntimeError!!! >:[')\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZPOAdtZMZTC"
      },
      "source": [
        "z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb_6M3O4MZTD"
      },
      "source": [
        "## More stuff\n",
        "\n",
        "La documentación del paquete de diferenciación automática (`autograd`) se encuentra en\n",
        "http://pytorch.org/docs/autograd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEQdXLb1MZTD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}